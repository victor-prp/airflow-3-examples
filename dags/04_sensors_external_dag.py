"""
External Systems & Sensors DAG - Airflow Training Step 4

This DAG demonstrates:
- FileSensor for waiting on external files
- TimeDeltaSensor for time-based waiting
- Custom sensors and polling patterns
- Timeout handling and sensor modes (poke vs reschedule)
- External system integration patterns
- Database sensors and API polling
- Reactive workflows triggered by external events

Key Learning Objectives:
- How to build reactive workflows that wait for external events
- Understanding sensor modes and their trade-offs
- Implementing custom sensors for specific use cases
- Handling timeouts and failure scenarios
- Best practices for external system integration
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.sensors.time_delta import TimeDeltaSensor
from airflow.sensors.base import BaseSensorOperator
from airflow.sensors.python import PythonSensor
from airflow.utils.context import Context
import os
import random
import time
import json

# Default arguments with sensor-specific settings
default_args = {
    'owner': 'airflow-training',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

# === SETUP FUNCTIONS ===

def setup_demo_environment():
    """
    Set up the demo environment for sensor testing.
    Creates directories and initial files for sensors to detect.
    """
    print("üîß Setting up demo environment for sensor testing...")
    
    # Create directories for file sensors
    directories = [
        '/tmp/airflow_demo/incoming',
        '/tmp/airflow_demo/processed', 
        '/tmp/airflow_demo/archive',
        '/tmp/airflow_demo/config'
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"üìÅ Created directory: {directory}")
    
    # Create a config file for sensors to monitor
    config_data = {
        'system_status': 'ready',
        'last_update': datetime.now().isoformat(),
        'processing_enabled': True,
        'batch_id': 'BATCH_001'
    }
    
    config_path = '/tmp/airflow_demo/config/system_config.json'
    with open(config_path, 'w') as f:
        json.dump(config_data, f, indent=2)
    
    print(f"‚öôÔ∏è Created config file: {config_path}")
    
    setup_summary = {
        'directories_created': len(directories),
        'config_file': config_path,
        'timestamp': datetime.now().isoformat(),
        'status': 'completed'
    }
    
    print(f"‚úÖ Demo environment setup completed: {setup_summary}")
    return setup_summary

def simulate_external_file_arrival():
    """
    Simulate an external system dropping a file.
    This would normally be done by an external system.
    """
    import time
    import random
    
    # Simulate processing time
    wait_time = random.randint(10, 30)  # 10-30 seconds
    print(f"‚è≥ Simulating external system processing (waiting {wait_time} seconds)...")
    
    # In real scenarios, this would be an external system
    # For demo, we'll create the file after a delay
    time.sleep(wait_time)
    
    # Create the file that sensors are waiting for
    file_path = '/tmp/airflow_demo/incoming/data_ready.txt'
    with open(file_path, 'w') as f:
        f.write(f"Data processed at {datetime.now().isoformat()}\n")
        f.write("This file signals that external data is ready for processing\n")
        f.write(f"Generated by external system simulation\n")
    
    print(f"üìÑ External file created: {file_path}")
    
    return {
        'file_path': file_path,
        'creation_time': datetime.now().isoformat(),
        'simulation_delay': wait_time,
        'status': 'file_created'
    }

# === CUSTOM SENSOR CLASSES ===

class CustomApiSensor(BaseSensorOperator):
    """
    Custom sensor that simulates checking an external API.
    Demonstrates how to create custom sensors.
    """
    
    def __init__(self, api_endpoint: str, expected_status: str = 'ready', **kwargs):
        super().__init__(**kwargs)
        self.api_endpoint = api_endpoint
        self.expected_status = expected_status
    
    def poke(self, context: Context) -> bool:
        """
        Check if the API returns the expected status.
        This is called repeatedly until it returns True or times out.
        """
        print(f"üåê Checking API endpoint: {self.api_endpoint}")
        
        # Simulate API call (in real scenario, use requests library)
        import random
        
        # Simulate API response with random status
        possible_statuses = ['not_ready', 'processing', 'ready']
        current_status = random.choice(possible_statuses)
        
        print(f"üì° API Response - Status: {current_status}")
        print(f"üéØ Expected Status: {self.expected_status}")
        
        if current_status == self.expected_status:
            print(f"‚úÖ API status matches expected: {current_status}")
            return True
        else:
            print(f"‚è≥ API not ready yet. Current: {current_status}, Expected: {self.expected_status}")
            return False

class DatabaseRecordSensor(BaseSensorOperator):
    """
    Custom sensor that waits for specific records in a database.
    Simulates checking for new data in a database table.
    """
    
    def __init__(self, table_name: str, min_records: int = 1, **kwargs):
        super().__init__(**kwargs)
        self.table_name = table_name
        self.min_records = min_records
    
    def poke(self, context: Context) -> bool:
        """
        Check if the database table has the minimum required records.
        """
        print(f"üóÑÔ∏è Checking table: {self.table_name}")
        print(f"üéØ Looking for at least {self.min_records} records")
        
        # Simulate database query (in real scenario, use database connection)
        import random
        current_records = random.randint(0, 10)
        
        print(f"üìä Current record count: {current_records}")
        
        if current_records >= self.min_records:
            print(f"‚úÖ Database has sufficient records: {current_records} >= {self.min_records}")
            return True
        else:
            print(f"‚è≥ Waiting for more records. Current: {current_records}, Need: {self.min_records}")
            return False

# === SENSOR CONDITION FUNCTIONS ===

def check_file_content_condition(file_path: str) -> bool:
    """
    Function for PythonSensor to check file content.
    More complex than just file existence.
    """
    print(f"üìÑ Checking content of file: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"‚ùå File does not exist: {file_path}")
        return False
    
    try:
        with open(file_path, 'r') as f:
            content = f.read()
        
        # Check if file contains expected content
        if 'READY_FOR_PROCESSING' in content:
            print(f"‚úÖ File contains expected content marker")
            return True
        else:
            print(f"‚è≥ File exists but doesn't contain ready marker")
            return False
            
    except Exception as e:
        print(f"‚ùå Error reading file: {e}")
        return False

def check_system_health() -> bool:
    """
    Function to check if external system is healthy.
    Simulates health check for downstream processing.
    """
    print("üîç Performing system health check...")
    
    # Simulate various health checks
    checks = {
        'api_responsive': random.choice([True, True, True, False]),  # 75% success
        'database_available': random.choice([True, True, False]),    # 66% success  
        'disk_space_ok': random.choice([True, True, True, True, False]),  # 80% success
        'memory_ok': True  # Always pass for demo
    }
    
    print("üè• Health check results:")
    for check_name, result in checks.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"   {check_name}: {status}")
    
    # All checks must pass
    all_healthy = all(checks.values())
    
    if all_healthy:
        print("‚úÖ All systems healthy - ready to proceed")
    else:
        print("‚ö†Ô∏è Some systems unhealthy - waiting...")
    
    return all_healthy

# === DATA PROCESSING FUNCTIONS ===

def process_sensor_triggered_data(**context):
    """
    Process data after sensors have confirmed external systems are ready.
    """
    task_instance = context['task_instance']
    
    print("‚öôÔ∏è === PROCESSING SENSOR-TRIGGERED DATA ===")
    
    # Get information from sensor tasks
    try:
        setup_info = task_instance.xcom_pull(task_ids='setup_environment')
        print(f"üîß Setup info: {setup_info}")
    except:
        print("‚ö†Ô∏è Could not retrieve setup info")
    
    # Simulate data processing
    processing_result = {
        'files_processed': 3,
        'records_processed': 15000,
        'processing_duration': '2.5 minutes',
        'output_location': '/tmp/airflow_demo/processed/',
        'data_quality_score': 94.7,
        'timestamp': datetime.now().isoformat(),
        'triggered_by': 'sensor_workflow'
    }
    
    print(f"üìä Processing completed: {processing_result}")
    return processing_result

def handle_sensor_timeout(**context):
    """
    Handle the case when sensors timeout.
    Demonstrates error handling for external system failures.
    """
    print("‚è∞ === HANDLING SENSOR TIMEOUT ===")
    print("üö® External systems did not become ready within timeout period")
    
    # In production, you might:
    # - Send alerts to monitoring systems
    # - Log to error tracking
    # - Trigger alternative workflows
    # - Create support tickets
    
    timeout_response = {
        'timeout_reason': 'external_system_unavailable',
        'recommended_action': 'check_external_systems',
        'alert_sent': True,
        'fallback_triggered': False,
        'timestamp': datetime.now().isoformat()
    }
    
    print(f"üîî Timeout response: {timeout_response}")
    return timeout_response

def cleanup_demo_files():
    """
    Clean up demo files created during sensor testing.
    """
    print("üßπ Cleaning up demo files...")
    
    import shutil
    cleanup_paths = [
        '/tmp/airflow_demo'
    ]
    
    files_removed = 0
    for path in cleanup_paths:
        if os.path.exists(path):
            if os.path.isdir(path):
                shutil.rmtree(path)
                print(f"üìÅ Removed directory: {path}")
            else:
                os.remove(path)
                print(f"üìÑ Removed file: {path}")
            files_removed += 1
    
    cleanup_summary = {
        'paths_cleaned': files_removed,
        'cleanup_time': datetime.now().isoformat(),
        'status': 'completed'
    }
    
    print(f"‚úÖ Cleanup completed: {cleanup_summary}")
    return cleanup_summary

# Create the DAG
dag = DAG(
    'sensors_external_systems_dag',
    default_args=default_args,
    description='DAG demonstrating sensors and external system integration patterns',
    schedule=timedelta(hours=4),  # Run every 4 hours
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['training', 'sensors', 'external-systems', 'reactive'],
)

# === TASK DEFINITIONS ===

# Setup the demo environment
setup_task = PythonOperator(
    task_id='setup_environment',
    python_callable=setup_demo_environment,
    dag=dag,
)

# Simulate external system creating a file (normally done externally)
simulate_external_task = PythonOperator(
    task_id='simulate_external_system',
    python_callable=simulate_external_file_arrival,
    dag=dag,
)

# File sensor - waits for a specific file to appear
file_sensor = FileSensor(
    task_id='wait_for_data_file',
    filepath='/tmp/airflow_demo/incoming/data_ready.txt',
    poke_interval=30,  # Check every 30 seconds
    timeout=300,       # Timeout after 5 minutes
    mode='poke',       # Stay running and check repeatedly
    dag=dag,
)

# Python sensor - more complex condition checking
python_sensor = PythonSensor(
    task_id='wait_for_processing_ready',
    python_callable=lambda: check_file_content_condition('/tmp/airflow_demo/config/system_config.json'),
    poke_interval=20,
    timeout=240,
    mode='poke',
    dag=dag,
)

# Time-based sensor - wait for a specific time delay
time_sensor = TimeDeltaSensor(
    task_id='wait_for_time_window',
    delta=timedelta(seconds=60),  # Wait 60 seconds from DAG start
    dag=dag,
)

# Custom API sensor
api_sensor = CustomApiSensor(
    task_id='wait_for_api_ready',
    api_endpoint='https://api.example.com/status',
    expected_status='ready',
    poke_interval=25,
    timeout=200,
    mode='poke',
    dag=dag,
)

# Custom database sensor
db_sensor = DatabaseRecordSensor(
    task_id='wait_for_database_records',
    table_name='external_data_feed',
    min_records=5,
    poke_interval=35,
    timeout=250,
    mode='poke',
    dag=dag,
)

# System health sensor
health_sensor = PythonSensor(
    task_id='wait_for_system_health',
    python_callable=check_system_health,
    poke_interval=40,
    timeout=200,
    mode='poke',
    dag=dag,
)

# Process data after all sensors are satisfied
process_task = PythonOperator(
    task_id='process_triggered_data',
    python_callable=process_sensor_triggered_data,
    dag=dag,
)

# Handle timeout scenarios
timeout_handler = PythonOperator(
    task_id='handle_timeout',
    python_callable=handle_sensor_timeout,
    trigger_rule='one_failed',  # Run if any upstream task fails
    dag=dag,
)

# Cleanup task
cleanup_task = PythonOperator(
    task_id='cleanup_demo_files',
    python_callable=cleanup_demo_files,
    trigger_rule='none_failed_min_one_success',  # Run regardless of some failures
    dag=dag,
)

# Demonstration of reschedule mode sensor (more resource-efficient)
reschedule_sensor = PythonSensor(
    task_id='reschedule_mode_demo',
    python_callable=lambda: random.choice([True, False, False]),  # 33% success rate
    poke_interval=60,
    timeout=180,
    mode='reschedule',  # Release worker slot between checks
    dag=dag,
)

# === WORKFLOW DEPENDENCIES ===

# Setup environment first
setup_task >> simulate_external_task

# Parallel sensor checks (all must succeed)
setup_task >> [
    file_sensor,
    python_sensor, 
    time_sensor,
    api_sensor,
    db_sensor,
    health_sensor,
    reschedule_sensor
]

# External system simulation feeds into file sensor
simulate_external_task >> file_sensor

# All sensors must complete before processing
[
    file_sensor,
    python_sensor,
    time_sensor, 
    api_sensor,
    db_sensor,
    health_sensor,
    reschedule_sensor
] >> process_task

# Handle timeout scenarios
[
    file_sensor,
    python_sensor,
    api_sensor,
    db_sensor,
    health_sensor,
    reschedule_sensor
] >> timeout_handler

# Cleanup runs after processing or timeout handling
[process_task, timeout_handler] >> cleanup_task
